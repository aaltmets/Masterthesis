{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaltmets/Masterthesis/blob/main/Copy_of_data_pulling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHeGUF7Odonn",
        "outputId": "b1e0fe3a-11a7-4f16-e0bd-357bd42a548f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweepy==4.10.1\n",
            "  Downloading tweepy-4.10.1-py3-none-any.whl (94 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from tweepy==4.10.1) (1.3.1)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.9/dist-packages (from tweepy==4.10.1) (2.27.1)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from tweepy==4.10.1) (3.2.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (2022.12.7)\n",
            "Installing collected packages: tweepy\n",
            "  Attempting uninstall: tweepy\n",
            "    Found existing installation: tweepy 4.13.0\n",
            "    Uninstalling tweepy-4.13.0:\n",
            "      Successfully uninstalled tweepy-4.13.0\n",
            "Successfully installed tweepy-4.10.1\n"
          ]
        }
      ],
      "source": [
        "#pip install tweepy==4.10.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "import configparser\n",
        "import time\n"
      ],
      "metadata": {
        "id": "Ot9g7DkcRn_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = 'your key'\n",
        "api_key_secret = 'your key'\n",
        "\n",
        "access_token = 'your token'\n",
        "access_token_secret = 'your token'\n",
        "\n",
        "bearer_token = 'your token '"
      ],
      "metadata": {
        "id": "OEOy8gyoSXjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#client = tweepy.Client(consumer_key=api_key, \n",
        "                                #consumer_secret=api_key_secret,\n",
        "                                #access_token=access_token, \n",
        "                                #access_token_secret=access_token_secret)\n",
        "\n",
        "client = tweepy.Client(bearer_token = bearer_token)"
      ],
      "metadata": {
        "id": "XBRuAiuYTK-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#previous query was covid travel, new coronavirus travel, then safe to travel, travel insurance and travel restriction for pre covid we also use virus travel \n",
        "#pre covid also has travel anxiety and travel again. Loop pulls tweets only for 15 mins then stops therefore nr of words is limited \n",
        "\n",
        "query = 'COVID travel -is:retweet lang:en has:geo' \n",
        "\n",
        "tweets = []\n",
        "for tweet in tweepy.Paginator(client.search_all_tweets, \n",
        "                                 query = query,\n",
        "                                 user_fields = ['username', 'name', 'public_metrics', 'description', 'location', 'verified', 'created_at', 'entities', 'profile_image_url'],\n",
        "                                 tweet_fields = ['created_at', 'geo', 'public_metrics', 'text', 'source', 'context_annotations'],\n",
        "                                 place_fields = ['geo','id','country', 'country_code'],\n",
        "                                 expansions = ['geo.place_id', 'author_id'],\n",
        "                                 start_time = '2019-01-01T00:00:00Z',\n",
        "                                 end_time = '2020-09-24T00:00:00Z',\n",
        "                              max_results=100):\n",
        "    time.sleep(1)\n",
        "    tweets.append(tweet)\n",
        "  \n"
      ],
      "metadata": {
        "id": "9f9BJk4ZTR9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "user_dict = {}\n",
        "\n",
        "\n",
        "\n",
        "for tweet in tweets:\n",
        "    # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
        "    for user in tweet.includes['users']:\n",
        "        user_dict[user.id] = {'username': user.username,\n",
        "                              'name': user.name, \n",
        "                              'followers': user.public_metrics['followers_count'],\n",
        "                              'following':user.public_metrics['following_count'],\n",
        "                              'tweets': user.public_metrics['tweet_count'],\n",
        "                              'description': user.description,\n",
        "                              'verified':user.verified,\n",
        "                              'profile_image_url': user.profile_image_url,\n",
        "                              'entities': user.entities,\n",
        "                              'created_at': user.created_at,\n",
        "                              'location': user.location\n",
        "                             }\n",
        "\n",
        "\n",
        "    for i in tweet.data:\n",
        "        # For each tweet, find the author's information\n",
        "        author_info = user_dict[i.author_id]\n",
        "        # Put all of the information we want to keep in a single dictionary for each tweet. It seems that geo.place_id is inside of dictionary to lets try to get it out and equal to place_dict \n",
        "     #place_info = place_dict[i.geo['place_id']\n",
        "        result.append({'author_id': i.author_id, \n",
        "                       'username': author_info['username'],\n",
        "                       'name': author_info['name'],\n",
        "                       'followers': author_info['followers'],\n",
        "                       'following': author_info['following'],\n",
        "                       'verified': author_info['verified'],\n",
        "                       'profile_image_url':author_info['profile_image_url'],\n",
        "                       'entities': author_info['entities'],\n",
        "                       'author_created_at':author_info['created_at'],\n",
        "                       'author_tweets': author_info['tweets'],\n",
        "                       'author_description': author_info['description'],\n",
        "                       'author_location': author_info['location'],\n",
        "                       'tweet_id': i.id,\n",
        "                       'text': i.text,\n",
        "                       'created_at': i.created_at,\n",
        "                       'geo': i.geo,\n",
        "                       'source':i.source,\n",
        "                       'retweets': i.public_metrics['retweet_count'],\n",
        "                       'replies': i.public_metrics['reply_count'],\n",
        "                       'likes': i.public_metrics['like_count'],\n",
        "                       'quote_count': i.public_metrics['quote_count']\n",
        "                      })\n",
        "\n",
        "# Change this list of dictionaries into a dataframe\n",
        "df = pd.DataFrame(result)"
      ],
      "metadata": {
        "id": "YpuCtOSFW5t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def try_literal_eval(e):\n",
        "    try:\n",
        "        return ast.literal_eval(e)\n",
        "    except ValueError:\n",
        "        return {'place_id': 0}\n",
        "\n",
        "df['geo'] = df['geo'].str['place_id']"
      ],
      "metadata": {
        "id": "AMYS32Q8EAVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I could not add location in normal way. received error. therefore created this workaround \n",
        "\n",
        "location = []\n",
        "\n",
        "for tweet in tweets: \n",
        "  for place in tweet.includes['places']:\n",
        "\n",
        "    location.append({'geo':place.id,\n",
        "                                  'country': place.country,\n",
        "                                  'country_code': place.country_code\n",
        "                                  })\n",
        "df_location = pd.DataFrame(location)"
      ],
      "metadata": {
        "id": "2r880WOXE0pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pd.merge(df, df_location, how = \"outer\", on =['geo'])"
      ],
      "metadata": {
        "id": "ua4qF8Dik62h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#because for some reseaon high number of tweets gets duplicated, remove duplicates \n",
        "result = result.drop_duplicates(subset=['tweet_id'])"
      ],
      "metadata": {
        "id": "WyOOSYh8nEof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hgg4vcvneFZ",
        "outputId": "58ea16ff-f8f3-432d-9b92-ac792cc976f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14946, 23)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_csv('file_name.csv')"
      ],
      "metadata": {
        "id": "GUJGC5LTnh_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = result.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "qMWB0egbpdro"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}