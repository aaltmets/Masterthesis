{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import urllib.request\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nltk.download('punkt')\n",
    "df = pd.read_csv('file.csv')\n",
    "df_c = df.dropna(subset = ['country'])\n",
    "\n",
    "top_10_countries = df_c.groupby('country_code')['tweet_id'].size().sort_values(ascending=False).head(10)\n",
    "print(top_10_countries)\n",
    "usa = df_c[df_c['country_code'] == 'US']\n",
    "\n",
    "usa_s = usa.groupby(['country_code']).agg(\n",
    "    {'username': 'nunique', 'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "usa_s\n",
    "usa_user = usa['username'].nunique()\n",
    "print(usa_user)\n",
    "uk = df_c[df_c['country_code'] == 'GB']\n",
    "uk_s = uk.groupby(['country_code']).agg(\n",
    "    {'username': 'nunique', 'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "uk_s\n",
    "canada = df_c[df_c['country_code'] == 'CA']\n",
    "canada_s = canada.groupby(['country_code']).agg(\n",
    "    {'username': 'nunique', 'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "canada_s\n",
    "df_users = df_c['username'].nunique()\n",
    "print(df_users)\n",
    "df_users_s = df_c['name'].nunique()\n",
    "print(df_users_s)\n",
    "india = df_c[df_c['country_code'] == 'IN']\n",
    "india_s = india.groupby(['country_code']).agg(\n",
    "    {'username': 'nunique', 'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "india_s\n",
    "countries_to_exclude = ['GB', 'US', 'IN', 'CA']\n",
    "row = df_c[~df_c['country_code'].isin(countries_to_exclude)]\n",
    "row['country'] = 'other'\n",
    "row_s = row.groupby(['country']).agg({'username': 'nunique', 'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "                                      'negative': 'mean'})\n",
    "row_s\n",
    "usa['date'] = pd.to_datetime(usa['date'])\n",
    "\n",
    "# Create a period index based on the date column\n",
    "usa = usa.set_index(pd.PeriodIndex(usa['date'], freq='Q'))\n",
    "\n",
    "# Calculate the mean score for each quarter\n",
    "df_avg = usa.groupby(usa.index)[['positive', 'negative', 'neutral']].mean()\n",
    "\n",
    "colors = {'positive': 'green', 'negative': 'red', 'neutral': 'orange'}\n",
    "# Create a line plot with separate lines for each score type\n",
    "ax = df_avg.plot.line(color=colors)\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "ax.set_title('Sentiment Scores Over Time USA')\n",
    "ax.set_xlabel('Quarter')\n",
    "ax.set_ylabel('Score')\n",
    "uk['date'] = pd.to_datetime(uk['date'])\n",
    "\n",
    "# Create a period index based on the date column\n",
    "uk = uk.set_index(pd.PeriodIndex(uk['date'], freq='Q'))\n",
    "\n",
    "# Calculate the mean score for each quarter\n",
    "df_avg = uk.groupby(uk.index)[['positive', 'negative', 'neutral']].mean()\n",
    "\n",
    "colors = {'positive': 'green', 'negative': 'red', 'neutral': 'orange'}\n",
    "# Create a line plot with separate lines for each score type\n",
    "ax = df_avg.plot.line(color=colors)\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "ax.set_title('Sentiment Scores Over Time UK')\n",
    "ax.set_xlabel('Quarter')\n",
    "ax.set_ylabel('Score')\n",
    "canada['date'] = pd.to_datetime(canada['date'])\n",
    "\n",
    "# Create a period index based on the date column\n",
    "canada = canada.set_index(pd.PeriodIndex(canada['date'], freq='Q'))\n",
    "\n",
    "# Calculate the mean score for each quarter\n",
    "df_avg = canada.groupby(canada.index)[['positive', 'negative', 'neutral']].mean()\n",
    "\n",
    "colors = {'positive': 'green', 'negative': 'red', 'neutral': 'orange'}\n",
    "# Create a line plot with separate lines for each score type\n",
    "ax = df_avg.plot.line(color=colors)\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "ax.set_title('Sentiment Scores Over Time Canada')\n",
    "ax.set_xlabel('Quarter')\n",
    "ax.set_ylabel('Score')\n",
    "india['date'] = pd.to_datetime(india['date'])\n",
    "\n",
    "# Create a period index based on the date column\n",
    "india = india.set_index(pd.PeriodIndex(india['date'], freq='Q'))\n",
    "\n",
    "# Calculate the mean score for each quarter\n",
    "df_avg = india.groupby(india.index)[['positive', 'negative', 'neutral']].mean()\n",
    "\n",
    "colors = {'positive': 'green', 'negative': 'red', 'neutral': 'orange'}\n",
    "# Create a line plot with separate lines for each score type\n",
    "ax = df_avg.plot.line(color=colors)\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "ax.set_title('Sentiment Scores Over Time India')\n",
    "ax.set_xlabel('Quarter')\n",
    "ax.set_ylabel('Score')\n",
    "row['date'] = pd.to_datetime(row['date'])\n",
    "\n",
    "# Create a period index based on the date column\n",
    "row = row.set_index(pd.PeriodIndex(row['date'], freq='Q'))\n",
    "\n",
    "# Calculate the mean score for each quarter\n",
    "df_avg = row.groupby(row.index)[['positive', 'negative', 'neutral']].mean()\n",
    "\n",
    "colors = {'positive': 'green', 'negative': 'red', 'neutral': 'orange'}\n",
    "# Create a line plot with separate lines for each score type\n",
    "ax = df_avg.plot.line(color=colors)\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "ax.set_title('Sentiment Scores Over Time Rest of the World (ROW)')\n",
    "ax.set_xlabel('Quarter')\n",
    "ax.set_ylabel('Score')\n",
    "#### pre covid\n",
    "#USA\n",
    "before_usa = usa[usa['date'] < '2020-01-30']\n",
    "\n",
    "before_usa_g = before_usa.groupby(['sentiment_label']).agg({'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "                                                            'negative': 'mean'})\n",
    "before_usa_g\n",
    "text_ub = before_usa['text_check']\n",
    "text_ub = ' '.join(text_ub)\n",
    "words_ub = word_tokenize(text_ub)\n",
    "\n",
    "words_no_punc_ub = []\n",
    "for word in words_ub:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_ub.append(word.lower())\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_ub = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_ub:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_ub.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_ub = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_ub:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_ub.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_ub)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('a) Pre-COVID USA')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"usa_before.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "before_usa_n = before_usa[before_usa['sentiment_label'] == 'negative']\n",
    "\n",
    "text_bun = before_usa_n['text_check']\n",
    "text_bun = ' '.join(text_bun)\n",
    "words_bun = word_tokenize(text_bun)\n",
    "\n",
    "words_no_punc_bun = []\n",
    "for word in words_bun:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bun.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bun = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bun:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bun.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bun = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bun:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bun.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bun)\n",
    "plt.title(\"Pre-COVID USA\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "before_usa_p = before_usa[before_usa['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bup = before_usa_p['text_check']\n",
    "text_bup = ' '.join(text_bup)\n",
    "words_bup = word_tokenize(text_bup)\n",
    "\n",
    "words_no_punc_bup = []\n",
    "for word in words_bup:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bup.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bup = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bup:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bup.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bup = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bup:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bup.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bup)\n",
    "plt.title(\"Pre-COVID USA\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### pre covid\n",
    "#UK\n",
    "before_uk = uk[uk['date'] < '2020-01-30']\n",
    "text_ukb = before_uk['text_check']\n",
    "text_ukb = ' '.join(text_ukb)\n",
    "words_ukb = word_tokenize(text_ukb)\n",
    "\n",
    "words_no_punc_ukb = []\n",
    "for word in words_ukb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_ukb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_ukb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_ukb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_ukb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_ukb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_ukb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_ukb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_ukb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('b) Pre-COVID UK')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"uk_before.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show\n",
    "\n",
    "before_uk_n = before_uk[before_uk['sentiment_label'] == 'negative']\n",
    "\n",
    "text_bukn = before_uk_n['text_check']\n",
    "text_bukn = ' '.join(text_bukn)\n",
    "words_bukn = word_tokenize(text_bukn)\n",
    "\n",
    "words_no_punc_bukn = []\n",
    "for word in words_bukn:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bukn.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bukn = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bukn:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bukn.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bukn = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bukn:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bukn.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bukn)\n",
    "plt.title(\"Pre-COVID UK\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "before_uk_p = before_uk[before_uk['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bukp = before_uk_p['text_check']\n",
    "text_bukp = ' '.join(text_bukp)\n",
    "words_bukp = word_tokenize(text_bukp)\n",
    "\n",
    "words_no_punc_bukp = []\n",
    "for word in words_bukp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bukp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bukp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bukp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bukp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bukp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bukp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bukp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bukp)\n",
    "plt.title(\"Pre-COVID UK\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "before_uk_g = before_uk.groupby(['sentiment_label']).agg({'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "                                                          'negative': 'mean'})\n",
    "before_uk_g\n",
    "#### pre covid\n",
    "#india\n",
    "before_india = india[india['date'] < '2020-01-30']\n",
    "before_india_g = before_india.groupby(['sentiment_label']).agg(\n",
    "    {'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "before_india_g\n",
    "text_ib = before_india['text_check']\n",
    "text_ib = ' '.join(text_ib)\n",
    "words_ib = word_tokenize(text_ib)\n",
    "\n",
    "words_no_punc_ib = []\n",
    "for word in words_ib:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_ib.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_ib = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_ib:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_ib.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_ib = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_ib:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_ib.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_ib)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('d) Pre-COVID India')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"india_before.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show\n",
    "before_i_n = before_india[before_india['sentiment_label'] == 'negative']\n",
    "\n",
    "text_bin = before_i_n['text_check']\n",
    "text_bin = ' '.join(text_bin)\n",
    "words_bin = word_tokenize(text_bin)\n",
    "\n",
    "words_no_punc_bin = []\n",
    "for word in words_bin:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bin.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bin = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bin:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bin.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bin = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bin:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bin.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bin)\n",
    "plt.title(\"Pre-COVID India\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "before_i_p = before_india[before_india['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bip = before_i_p['text_check']\n",
    "text_bip = ' '.join(text_bip)\n",
    "words_bip = word_tokenize(text_bip)\n",
    "\n",
    "words_no_punc_bip = []\n",
    "for word in words_bip:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bip.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bip = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bip:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bip.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bip = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bip:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bip.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bip)\n",
    "plt.title(\"Pre-COVID India\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### pre covid\n",
    "#Canada\n",
    "before_canada = canada[canada['date'] < '2020-01-30']\n",
    "before_canada_g = before_canada.groupby(['sentiment_label']).agg(\n",
    "    {'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "before_canada_g\n",
    "text_cb = before_canada['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('c) Pre-COVID Canada')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"canada_before.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show\n",
    "before_c_p = before_canada[before_canada['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bcp = before_c_p['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Pre-COVID Canada\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "before_c_p = before_canada[before_canada['sentiment_label'] == 'negative']\n",
    "\n",
    "text_bcp = before_c_p['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Pre-COVID Canada\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### pre covid\n",
    "#ROW\n",
    "before_row = row[row['date'] < '2020-01-30']\n",
    "before_row_g = before_row.groupby(['sentiment_label']).agg({'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "                                                            'negative': 'mean'})\n",
    "before_row_g\n",
    "text_cb = before_row['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('d) Pre-COVID Rest of the world (ROW)')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"row_before.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "before_c_p = before_row[before_row['sentiment_label'] == 'negative']\n",
    "\n",
    "text_bcp = before_c_p['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Pre-COVID Rest of the World (ROW)\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "before_c_p = before_row[before_row['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bcp = before_c_p['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Pre-COVID Rest of the World (ROW)\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### pre lockdown\n",
    "#usa\n",
    "before_l_usa = usa[(usa['date'] >= '2020-01-30') & (usa['date'] < '2020-03-15')]\n",
    "before_l_usa_g = before_l_usa.groupby(['sentiment_label']).agg(\n",
    "    {'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "before_l_usa_g\n",
    "text_cb = before_l_usa['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('a) Pre-lockdown USA')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"usa_before_l.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "before_l_usa_n = before_l_usa[before_l_usa['sentiment_label'] == 'negative']\n",
    "\n",
    "text_bcp = before_l_usa_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Pre-lockdown USA\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "before_l_usa_n = before_l_usa[before_l_usa['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bcp = before_l_usa_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Pre-lockdown USA\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### pre lockdown\n",
    "#UK\n",
    "before_l_uk = uk[(uk['date'] >= '2020-01-30') & (uk['date'] < '2020-03-23')]\n",
    "before_l_uk_g = before_l_uk.groupby(['sentiment_label']).agg(\n",
    "    {'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "before_l_uk_g\n",
    "text_cb = before_l_uk['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('b) Pre-lockdown UK')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"uk_before_l.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "before_l_uk_n = before_l_uk[before_l_uk['sentiment_label'] == 'negative']\n",
    "\n",
    "text_bcp = before_l_uk_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Pre-lockdown UK\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "before_l_uk_n = before_l_uk[before_l_uk['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bcp = before_l_uk_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Pre-lockdown UK\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### pre lockdown\n",
    "#Canada\n",
    "before_l_canada = canada[(canada['date'] >= '2020-01-30') & (canada['date'] < '2020-03-23')]\n",
    "before_l_canada_g = before_l_canada.groupby(['sentiment_label']).agg(\n",
    "    {'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "before_l_canada_g\n",
    "text_cb = before_l_canada['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('c) Pre-state of emergency Canada')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"canada_before_l.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "before_l_canada_n = before_l_canada[before_l_canada['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bcp = before_l_canada_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Pre-state of emergency Canada\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### pre lockdown\n",
    "#India\n",
    "before_l_india = india[(india['date'] >= '2020-01-30') & (india['date'] < '2020-03-25')]\n",
    "before_l_india_g = before_l_india.groupby(['sentiment_label']).agg(\n",
    "    {'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "before_l_india_g\n",
    "text_cb = before_l_india['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('d) Pre-lockdown India')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"india_before_l.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "before_l_india_n = before_l_india[before_l_india['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bcp = before_l_india_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Pre-lockdown India\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### pre lockdown\n",
    "#ROW\n",
    "before_l_row = row[(row['date'] >= '2020-01-30') & (row['date'] < '2020-03-15')]\n",
    "before_l_row_g = before_l_row.groupby(['sentiment_label']).agg(\n",
    "    {'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "before_l_row_g\n",
    "text_cb = before_l_row['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('e) Pre-lockdown Rest of the World (ROW)')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"row_before_l.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "before_l_row_n = before_l_row[before_l_row['sentiment_label'] == 'negative']\n",
    "\n",
    "text_bcp = before_l_row_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Pre-lockdown Rest of the World (ROW)\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### lockdown\n",
    "#usa\n",
    "during_usa = usa[(usa['date'] >= '2020-03-15') & (usa['date'] < '2020-06-16')]\n",
    "during_usa_g = during_usa.groupby(['sentiment_label']).agg({'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "                                                            'negative': 'mean'})\n",
    "during_usa_g\n",
    "text_cb = during_usa['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('a) Lockdown USA')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"usa_during.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "during_usa_n = during_usa[during_usa['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bcp = during_usa_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Lockdown USA\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### lockdown\n",
    "#UK\n",
    "during_uk = uk[(uk['date'] >= '2020-03-23') & (uk['date'] < '2020-08-02')]\n",
    "during_uk_g = during_uk.groupby(['sentiment_label']).agg({'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "                                                          'negative': 'mean'})\n",
    "during_uk_g\n",
    "text_cb = during_uk['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('b) Lockdown UK')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"uk_during.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "during_uk_n = during_uk[during_uk['sentiment_label'] == 'negative']\n",
    "\n",
    "text_bcp = during_uk_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Lockdown UK\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### lockdown\n",
    "#india\n",
    "during_india = india[(india['date'] >= '2020-03-25') & (india['date'] < '2020-06-01')]\n",
    "during_india_g = during_india.groupby(['sentiment_label']).agg(\n",
    "    {'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "during_india_g\n",
    "text_cb = during_india['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('d) Lockdown India')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"during_india.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "during_india_n = during_india[during_india['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bcp = during_india_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Lockdown India\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### lockdown\n",
    "#canada\n",
    "during_canada = canada[(canada['date'] >= '2020-03-23') & (canada['date'] < '2022-02-25')]\n",
    "during_canada_g = during_canada.groupby(['sentiment_label']).agg(\n",
    "    {'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "during_canada_g\n",
    "text_cb = during_canada['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('c) State pf emergency Canada')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"during_canada.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "during_canada_n = during_canada[during_canada['sentiment_label'] == 'negative']\n",
    "\n",
    "text_bcp = during_canada_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"State of emergency Canada\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### lockdown\n",
    "#ROW\n",
    "during_row = row[(row['date'] >= '2020-03-15') & (row['date'] < '2020-06-16')]\n",
    "during_row_g = during_row.groupby(['sentiment_label']).agg({'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "                                                            'negative': 'mean'})\n",
    "during_row_g\n",
    "text_cb = during_row['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('e) Lockdown Rest of the World (ROW)')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"during_row.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "during_row_n = during_row[during_row['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bcp = during_row_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Lockdown Rest of the World (ROW)\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### post\n",
    "#USA\n",
    "post_usa = usa[usa['date'] >= '2020-06-16']\n",
    "post_usa_g = post_usa.groupby(['sentiment_label']).agg({'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "                                                        'negative': 'mean'})\n",
    "post_usa_g\n",
    "text_cb = post_usa['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('a) Pos-lockdown USA')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"post_usa.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "post_usa_n = post_usa[post_usa['sentiment_label'] == 'negative']\n",
    "\n",
    "text_bcp = post_usa_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Post-lockdown USA\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### post\n",
    "#UK\n",
    "post_uk = uk[uk['date'] >= '2020-08-02']\n",
    "post_uk_g = post_uk.groupby(['sentiment_label']).agg({'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "                                                      'negative': 'mean'})\n",
    "post_uk_g\n",
    "text_cb = post_uk['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('b) Pos-lockdown UK')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"post_uk.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "post_uk_n = post_uk[post_uk['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bcp = post_uk_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Post-lockdown UK\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### post\n",
    "#canada\n",
    "post_canada = canada[canada['date'] >= '2022-02-25']\n",
    "post_canada_g = post_canada.groupby(['sentiment_label']).agg(\n",
    "    {'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "     'negative': 'mean'})\n",
    "post_canada_g\n",
    "text_cb = post_canada['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('c) Post-state of emergency Canada')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"post_can.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "post_canada_n = post_canada[post_canada['sentiment_label'] == 'negative']\n",
    "\n",
    "text_bcp = post_canada_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Post-state of emergency Canada\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### post\n",
    "#india\n",
    "post_india = india[india['date'] >= '2020-06-01']\n",
    "post_india_g = post_india.groupby(['sentiment_label']).agg({'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "                                                            'negative': 'mean'})\n",
    "post_india_g\n",
    "text_cb = post_india['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('d) Post-lockdown India ')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"post_india.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "post_india_n = post_india[post_india['sentiment_label'] == 'positive']\n",
    "\n",
    "text_bcp = post_india_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Post-lockdown India\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n",
    "#### post\n",
    "#row\n",
    "post_row = row[row['date'] >= '2020-06-16']\n",
    "post_row_g = post_row.groupby(['sentiment_label']).agg({'tweet_id': 'count', 'positive': 'mean', 'neutral': 'mean',\n",
    "                                                        'negative': 'mean'})\n",
    "post_row_g\n",
    "text_cb = post_row['text_check']\n",
    "text_cb = ' '.join(text_cb)\n",
    "words_cb = word_tokenize(text_cb)\n",
    "\n",
    "words_no_punc_cb = []\n",
    "for word in words_cb:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_cb.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_cb = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_cb:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_cb.append(word)\n",
    "\n",
    "#Convert word list to a single string\n",
    "clean_words_string = \" \".join(clean_words_cb)\n",
    "\n",
    "#generating the wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(clean_words_string)\n",
    "\n",
    "#plot the wordcloud\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('e) Post-lockdown Rest of the World (ROW)')\n",
    "#to remove the axis value\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"postrow.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "post_row_n = post_row[post_row['sentiment_label'] == 'negative']\n",
    "\n",
    "text_bcp = post_row_n['text_check']\n",
    "text_bcp = ' '.join(text_bcp)\n",
    "words_bcp = word_tokenize(text_bcp)\n",
    "\n",
    "words_no_punc_bcp = []\n",
    "for word in words_bcp:\n",
    "    if word.isalpha():\n",
    "        words_no_punc_bcp.append(word.lower())\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "#Update the stopwords list\n",
    "stopwords_list.extend([\"said\", \"one\", \"like\", \"came\", \"back\", \"cant\", \"im\", \"amp\", \"dont\", \"get\"])\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#create an empty list to store clean words\n",
    "clean_words_bcp = []\n",
    "\n",
    "#Iterate through the words_no_punc list and add non stopwords to the new clean_words list\n",
    "for word in words_no_punc_bcp:\n",
    "    if word not in stopwords_list:\n",
    "        clean_words_bcp.append(word)\n",
    "\n",
    "#find the frequency of words\n",
    "fdist = FreqDist(clean_words_bcp)\n",
    "plt.title(\"Post-lockdown Rest of the World (ROW)\")\n",
    "#Plot the 10 most common words\n",
    "for i, (word, freq) in enumerate(fdist.most_common(30)):\n",
    "    plt.annotate(f\"{freq}\", xy=(i, freq), xytext=(i, freq + 5), ha='center')\n",
    "\n",
    "fdist.plot(10)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
